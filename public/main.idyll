[Title
  title:"Building complex reasoning skills online through open-ended activities"
  authors:`[{name:"Nick Barr", link:"https://www.nsbarr.com"}, {name:"Scott Farrar", link: "https://twitter.com/farrarscott"}, {name:"Brian Johnsrud", link:"https://www.linkedin.com/in/brianjohnsrud/"}, {name:"May-Li Khoe", link:"http://www.maylikhoe.com"}, {name:"Andy Matuschak", link:"https://andymatuschak.org"}]`
  date:"August 24, 2018"
/]

# Abstract

On Khan Academy and other online learning platforms, students build skills by practicing with interactive exercises that deliver instant feedback. While these activities work well for factual and procedural knowledge, students also need to learn complex reasoning skills, including: literary analysis, scientific inquiry, critical thinking, mathematical modeling. To develop these skills, students need practice explaining and justifying their ideas. They also need feedback beyond a grade; they need specific, actionable feedback to guide their revision and iteration.

[Aside]
![illustration of complex forms of thought](images/complex-thought-noborder.png)
[/Aside]

[KeyQuote]How might we design open-ended activities for online platforms which help students develop complex reasoning skills while still delivering rapid feedback?[/KeyQuote]

The Early Product Development group at Khan Academy began investigating this question in early 2017. Since then, we’ve worked with teachers across the country to run classroom pilots and to iterate on our solution.

We've created a prototype learning platform that offers students rapid feedback by choreographing an intricate dance between students, their respective ideas, expert-authored model work, and lightweight teacher facilitation. There’s still much to do, but now that our latest pilots have received glowing reviews from teachers and students, we’re thrilled to share our approach.

[Hairline /]

# 1. Rich activities require rich feedback 

[p]*"Why is there a big spike in our data on this plot?"*[/p]

[p]*"Since globalization, what's changed? What's stayed the same?"* [/p
]
[p]*"What are the most important causes of structural inequality?"*[/p]

[p]*“How does water quality affect fish populations?”* [/p]

The most challenging and meaningful questions have no straightforward answers. They don't come with a simple set of rules that always yields a solution. These open-ended problems are "ill-defined,"[CitationRef /] and answering them requires *complex reasoning skills*: skills that build on extended inquiry, creativity, and critical thinking[CitationRef /].

[Aside][Citation]
 [Chi, M. T. H., & Glaser, R. (1985). Problem-solving ability. In R. J. Sternberg (Ed.), Human abilities (pp. 227–250). New York: W. H. Freeman and Company.](https://chilab.asu.edu/sites/all/themes/chilab/public/publication_files/ChiGlaser10.pdf)
[/Citation]
[Citation]
[Webb, N. L. (1999). Alignment of Science and Mathematics Standards and Assessments in Four States. Research Monograph No. 18. National Institute for Science Education, Madison WI.; Council of Chief State School Officers, Washington, DC.](https://scholar.google.com/scholar?cluster=3322723789698838463&hl=en&as_sdt=0,5)
[/Citation]
[/Aside]

Such skills are part of every core subject's curricular standards, but exam results indicate students struggle to develop these complex reasoning skills.

* [Math students must](http://www.corestandards.org/Math/Content/HSM/) develop modeling skills like "choosing and using appropriate mathematics and statistics … to improve decisions." But 41% of California’s 11th graders [could not reliably demonstrate this type of skill](https://caaspp.cde.ca.gov/sb2017/ViewReport?ps=true&lstTestYear=&lstTestType=B&lstCounty=00&lstDistrict=00000&lstSchool=0000000) on 2017's state test.
* [Science students must](http://www.nap.edu/read/13165/page/69) “construct their own explanations of phenomena.” But [58% of students got 0/7 points](https://twitter.com/AP_Trevor/status/1007321796784345089) on this type of task on 2018's AP Physics 1 test.
* [Social studies students must](http://www.corestandards.org/ELA-Literacy/RH/11-12) "assess  authors' claims, reasoning, and evidence." But in the 2017 AP US History exam, [49% of students failed](https://apscore.collegeboard.org/scores/about-ap-scores/score-distributions/), with this skill [cited](https://secure-media.collegeboard.org/digitalServices/pdf/ap/ap17-chief-reader-report-united-states-history.pdf) as a common knowledge gap.

[Aside]
A complex reasoning skill—the ability to *construct and critique arguments using evidence*—is the only skill shared between all core subjects' standards: [Common Core Math](http://www.corestandards.org/Math/Practice/MP3/), [Common Core English Language Arts](http://www.corestandards.org/ELA-Literacy/CCRA/W/), and [Next Generation Science](http://www.nap.edu/read/13165/page/69).
[/Aside]

To successfully develop complex reasoning skills like these, students need plenty of practice, delivering plenty of feedback[CitationRef /].

[Aside]
[Citation]
[Hattie, J., & Timperley, H. (2016). The Power of Feedback. Review of Educational Research, 77(1), 81–112. http://doi.org/10.3102/003465430298487](https://scholar.google.com/scholar?cluster=14300878074878976792&hl=en&as_sdt=0,5)
[/Citation]

[Citation]
The academic literature refers to right–wrong feedback as "knowledge of result." See: [Shute, V. J. (2008). Focus on Formative Feedback. Review of Educational Research, 78(1), 153–189.](https://scholar.google.com/scholar?cluster=7594307164316851197&hl=en&as_sdt=0,5)
[/Citation]
[/Aside]

When it comes to developing complex reasoning skills, simpler tasks like multiple-choice with right vs. wrong feedback[CitationRef /] fall short. They capture only an answer, not the nuanced thought which produced it. That means these types of exercises can't possibly give detailed feedback on the student’s thought processes: their thinking never got recorded in the first place. 

Instead, students need *open-ended activities* that ask them to *explain and justify their ideas*. Those activities must *deliver rapid feeback* that facilitates sense-making[CitationRef /] by asking students to elaborate, validate, and ultimately revise their ideas.[CitationRef /]

[Aside]
[Citation]
[Schoenfeld, A. H. (2016). Learning to Think Mathematically: Problem Solving, Metacognition, and Sense Making in Mathematics (Reprint). Journal of Education, 196(2), 1–38. http://doi.org/10.1177/002205741619600202](https://scholar.google.com/scholar?cluster=12324924934125279820&hl=en&as_sdt=0,5)
[/Citation]
[Citation]
[Xun, G. E., & Land, S. M. (2004). A conceptual framework for scaffolding ill-structured problem-solving processes using question prompts and peer interactions. Educational Technology Research and Development, 52(2), 5–22. http://doi.org/10.1007/BF02504836](https://scholar.google.com/scholar?cluster=2788378003420792530&hl=en&as_sdt=0,5)
[/Citation]
[/Aside]

Unfortunately, today's solutions for facilitating these kinds of activities all present significant challenges, which we'll now describe.

# 2. Today’s solutions and their challenges

To inform our approach, we examined how teachers and students conduct open-ended activities both in classrooms and online today.

Proficient teachers with small class sizes orchestrate group discussions to develop complex reasoning skills[CitationRef /][CitationRef /]. A quick vignette illustrates how that might look:

[Aside]
[Citation]
[Stein, M. K., Engle, R. A., Smith, M. S., & Hughes, E. K. (2008). Orchestrating Productive Mathematical Discussions: Five Practices for Helping Teachers Move Beyond Show and Tell. Mathematical Thinking and Learning, 10(4), 313–340. http://doi.org/10.1080/10986060802229675](https://scholar.google.com/scholar?cluster=14875716268307834673&hl=en&as_sdt=0,5)
[/Citation]
[Citation]
[Brookfield, Stephen D. Teaching for critical thinking: Tools and techniques to help students question their assumptions. John Wiley & Sons, 2011.](https://scholar.google.com/scholar?cluster=1845911061950561464&hl=en&as_sdt=0,5)
[/Citation]
[/Aside]

> The teacher begins class discussion: "What are the most significant causes of structural inequality?" Students write briefly on their own as she peppers in advice: “Draw evidence to support your argument from the primary sources in your packets.” Then the teacher has each person share with their neighbor, focusing their attention: “Which evidence did your partner use? How did they build their argument from it?” The teacher invites pairs to share their ideas, lightly prompting them to draw out certain points and disagreements. She summarizes the discussion on the board, emphasizing key concepts and connecting them to broader ideas.

In our time spent with teachers, we've learned just how challenging it can be to successfully orchestrate these activities. Teachers must devote considerable energy to maintaining discussion norms and facilitating conversational structures. These conversations mostly engage one speaker at a time; other students may disengage. While students share individually during the paired segment, most students' ideas won't be shared or discussed in the whole-class setting. And since every student is participating in the same discussion, it can be challenging to create enough differentiation to accommodate widely-varying student abilities.

Some teachers try to mitigate these challenges through online forum software.[CitationRef /] This asynchronous environment has some advantages: every student has the opportunity to speak; students can explore and engage with more ideas; and students have time to formulate their thoughts before speaking. Unfortunately, because forums have no structure to shape discussion pedagogically, they often fail to help students think more deeply[CitationRef /]—even when teachers intervene with occasional replies[CitationRef /].

[Aside]
[Citation]
[Brookfield, S. D., & Preskill, S. (2012). Discussion as a way of teaching. John Wiley & Sons.](https://scholar.google.com/scholar?cluster=9894785856214699219&hl=en&as_sdt=0,5)
[/Citation]
[Citation]
[Christopher, M. M., Thomas, J. A., & Runnels, M. K. T. (2010). Raising the bar: Encouraging high level thinking in online discussion forums. Roeper Review, 26(3), 166–171. http://doi.org/10.1080/02783190409554262](https://scholar.google.com/scholar?cluster=7364493968469079885&hl=en&as_sdt=0,5)
[/Citation]
[Citation]
[Mazzolini, M., & Maddison, S. (2007). When to jump in: The role of the instructor in online discussion forums. Computers & Education, 49(2), 193–213. http://doi.org/10.1016/j.compedu.2005.06.011](https://scholar.google.com/scholar?cluster=11889619368049320078&hl=en&as_sdt=0,5)
[/Citation]

One teacher we visited gestured to a huge stack of papers he’d just finished reading. “I have 196 students. That’s 16 hours right there.”
[/Aside]

Another way students might develop complex reasoning skills is through writing activities. The teacher collects student work (an essay, a project, a poster) then returns it with detailed feedback later. This way, teachers can assign different tasks to students of different proficiency, and each student ultimately receives direct feedback on their work. Unfortunately, teachers tell us this process consumes untold hours, so they can’t often give individualized feedback on these burdensome open-ended activities. Worse: students may wait weeks to receive feedback, by which time they’ve moved on to their next project. As such, they focus on their grade instead of considering and implementing the delayed feedback.

Peer grading can deliver feedback more rapidly. Peer grading asks students to evaluate each others' work, often using a rubric, then delivers a grade to each student based on those peer assessments. Systems like [PeerStudio](https://www.peerstudio.org) have established methods[CitationRef /] for ensuring that student-generated grades are reasonably accurate. These evaluations can help students assess their progress, and a good rubric can help students understand the activity’s learning objectives.

[Aside]
[Citation]
[Kulkarni, C. E., Bernstein, M. S., & Klemmer, S. R. (2015). PeerStudio: Rapid Peer Feedback Emphasizes Revision and Improves Performance. the Second (2015) ACM Conference (pp. 75–84). New York, New York, USA: ACM. http://doi.org/10.1145/2724660.2724670](https://scholar.google.com/scholar?cluster=1866805966852262903&hl=en&as_sdt=0,5)
[/Citation]
[/Aside]

But peer grading is limited to *assessing* skills. As expert teachers know, *assessing* a skill is different from *developing* a skill.

To understand the distinction, look at the following section of the [SAT's writing rubric for "analysis"](https://collegereadiness.collegeboard.org/sat/scores/understanding-scores/essay):


[AcrossAllColumns]
[RawTable]
  [tr]
    [td][strong]1/4 points[/strong][/td]
    [td][strong]2/4 points[/strong][/td]
    [td][strong]3/4 points[/strong][/td]
    [td][strong]4/4 points[/strong][/td]
  [/tr]
  [tr]
    [td]Offers little or no analysis or ineffective analysis of the source text and demonstrates little or no understanding of the analytic task.[/td]
    [td]Offers limited analysis of the source text and demonstrates only partial understanding of the analytical task.[/td]
    [td]Offers an effective analysis of the source text and demonstrates an understanding of the analytical task.[/td]
    [td]Offers an insightful analysis of the source text and demonstrates a sophisticated understanding of the analytical task.[/td]
  [/tr]
[/RawTable]
[/AcrossAllColumns]

Imagine that you're told you scored 2 points, but you want to build your analytical skills so that you’ll earn 3 points next time. What should you do? Because this rubric only *assesses* skills, but doesn't give feedback on how to *develop* them, the steps needed to improve are unclear.

Some peer grading systems do ask peers to send their classmates explicit feedback or advice, like what the teachers often do themselves, if they can find enough time. External research and our own teacher interviews suggest that this approach sometimes yields useful feedback—but not consistently.

[SemiHairline /]

None of the solutions we surveyed reliably and rapidly help students with the development of complex reasoning skills.

Writing activities often feel heavy for both students and teachers; a better solution would feel *light*, like a fluid conversation.

Classroom discussions often fail to involve all voices, and practice work often feels disposable; a better solution would show every student that their thoughts are *meaningful*.

Practice activities and assessments can feel isolated or alienating; a better solution would carry the *social* energy and diverse ideas of classroom discussions.

# 3. Our solution

[Aside]
![illustration of student and teacher interactions](images/our-solution-noborder.png)
[/Aside]

We’ve developed a prototype for an online platform to help students develop complex reasoning skills through open-ended activities that feel *light*, *meaningful*, and *social*.

This platform provides students with rapid feedback by arranging scaffolded interactions between students, their respective ideas, expert-authored model work, and lightweight teacher facilitation. 

Our design is the result of many months of exploration. Throughout, we collaborated with teachers who were generous with their time and expertise. We interviewed more than 50 educators in social studies, English language arts, math, and science. Even as we drew inspiration from classroom practices and ran classroom pilots, we remained interested in how our work might build bridges between and beyond classrooms and disciplines, exposing students to a broader set of ideas and people.

Our ideas began as simple paper prototypes and blossomed into classroom pilots that we ran with more than 600 students across the country, focused on public middle and high schools. We tracked the progress of each iteration based on quantitative survey data and qualitative feedback from pilot teachers and students, as well as by reading through logs of students' experiences to trace the evolution of their thinking as they moved through the activities. We used this data informally as an input to our iterative design process; a formal efficacy study was out of scope for our investigation.

Now that our latest pilots have received glowing reviews from teachers and students, we’re thrilled to share the platform and how it works. We'll first walk through the student experience: _write_, _react_, then _revise_. Then we'll discuss the core features designed to help students develop complex reasoning skills. 

[TwoUpImage imageURL:"images/write.png" heading:"A. Write: responding to open-ended problems"]

Students begin by taking a few minutes to draft a response to an open-ended question. There is no single best answer to the question in that image—and that’s exactly why it’s interesting. Rich questions draw out the nuances in student thought. By making more of that thought visible, we begin to empower peers, teachers, or even the student themselves to react and extend it. 

At this point, we're just asking students for a first draft. We encourage students to capture their rough thinking quickly; they'll iterate later. We build a space with [low floors, high ceilings, and wide walls](https://design.blog/2016/08/25/mitchel-resnick-designing-for-wide-walls/)[CitationRef /]: accessible for students with little proficiency, intriguing even for professionals in the field, and open to a variety of approaches.

The student context varied a great deal across our pilots: some teachers conducted these activities in class; others assigned them as homework. Sometimes students used school-provided laptops; others used a mobile interface on their personal smartphones. The core of the experience remained the same in all these cases.

[Citation]
[Resnick, M., & Silverman, B. (2005). Some reflections on designing construction kits for kids. Proceeding of the 2005 conference (pp. 117–122). New York, New York, USA: ACM. http://doi.org/10.1145/1109540.1109556](https://scholar.google.com/scholar?cluster=4999215177527413365&hl=en&as_sdt=0,5)
[/Citation]

[/TwoUpImage]

[TwoUpImage imageURL:"images/respond.png" heading:"B. React: engaging with others’ ideas"]

Once students have written a response of their own, we help them develop their thinking by facilitating structured interactions with ideas written by both peers and experts. Those peers might be their familiar classmates, or they might be students at different schools; they might be tackling the activity simultaneously, or they might engage asynchronously across a few days.

The pedagogical principle is that students build their understanding of the skill by effortfully engaging with others' use of that skill. To quote one middle school math student from our pilot:

> Seeing someone else’s work helped me realize how theirs was wrong—then I realized they did the same thing I did! Then I went back and fixed my work.

Just like in a good classroom discussion, students don’t listen passively to their peer’s ideas: they respond, elaborate, and synthesize them. We ignite conversation between students that highlights connections and clarifies conceptions. In so doing, we hope to echo the  classroom inspiration described by Mary Kay Stein and her collaborators[CitationRef /]:

> Sometimes two students find that they agree with each other. At other times, their ways of reasoning may differ, but both are correct. This experience provides an opportunity to find out how two different pathways can lead to the same understanding or solution. Finally, students can find that their reasoning differs from that of other students and that they disagree on a fundamental idea or a solution to a problem, thus revealing the need to figure out whose reasoning is correct. All of these scenarios offer opportunities for students to enhance their understanding…

[Citation]
 [Stein, M. K., Engle, R. A., Smith, M. S., & Hughes, E. K. (2008). Orchestrating Productive Mathematical Discussions: Five Practices for Helping Teachers Move Beyond Show and Tell. Mathematical Thinking and Learning, 10(4), 313–340. http://doi.org/10.1080/10986060802229675](https://scholar.google.com/scholar?cluster=14875716268307834673&hl=en&as_sdt=0,5)
[/Citation]

[/TwoUpImage]

[TwoUpImage imageURL:"images/revise.png" heading:"C. Revise: refining ideas and assessing development"]

Once students have responded to a few specifically-matched peers, we unlock a broader experience focused on reflection and revision. At this point, students can browse their whole class’s work, read the replies they’ve received from peers, and revise their original answer based on what they’ve learned.

The revision step offers students an opportunity to review and integrate new insights they’ve gained from the activity into what they already understand. Plus, students can revise their work as many times as they like. Teachers and students together bound the activity’s duration and decide when to move on; revision is an opportunity to submit *another* draft—not a "final" draft.[CitationRef hidden:true /]

While students do receive replies from their peers in this phase, our model doesn't rely exclusively on peer feedback to help students learn. In fact, our observations and interviews suggest that the act of effortfully *writing* replies to others' work is more reliably helpful than *receiving* replies from others. Peer feedback is *sometimes* helpful, of course—but even when it's not, those replies are still quite important for student engagement. Students tell us they take the activity more seriously because they know their classmates will be reading them.

[Citation][Fitzgerald, J. (1987). Research on Revision in Writing. Review of Educational Research, 57(4), 481–506. http://doi.org/10.3102/00346543057004481](https://scholar.google.com/scholar?cluster=12711957255674593856&hl=en&as_sdt=0,5)
[/Citation]

[/TwoUpImage]

# 4. Core features

## Sentence starters

[em]"Where before there was a spectator, let there now be a participant." —Jerome Bruner[CitationRef /][/em]

[Aside][Citation]
[Bruner, J. S. (1983). Child’s talk: Learning to use language. New York: Norton.](https://scholar.google.com/scholar?cluster=11855798828756889764&hl=en&as_sdt=0,5)
[/Citation][/Aside]

With open-ended questions like the ones we’re asking, students wrestle simultaneously with many different concepts, so we need to offer some structured scaffolds[CitationRef /] to help guide their interactions. We experimented with a variety of structures which might push students to participate in thoughtful ways with peers’ and experts’ ideas. 

[Aside]
[Citation]
[Wood, D., Bruner, J. S., & Ross, G. (1976). The role of tutoring in problem solving. Journal of child psychology and psychiatry, 17(2), 89-100. http://doi.org/10.1111/j.1469-7610.1976.tb00381.x](https://scholar.google.com/scholar?cluster=17121182529482739895&hl=en&as_sdt=0,5)
[/Citation]
[/Aside]

Xun Ge and Susan Land suggest[CitationRef /] that we can help students by asking *elaborative questions* like “Why is X important?” or “How does X affect Y?” These types of questions “elicit explanatory responses and high-level thinking elaboration, and thus are effective in facilitating knowledge building of learners of various age groups.”

[Aside]
[Citation]
[Xun, G. E., & Land, S. M. (2004). A conceptual framework for scaffolding ill-structured problem-solving processes using question prompts and peer interactions. Educational Technology Research and Development, 52(2), 5–22.](https://scholar.google.com/scholar?cluster=2788378003420792530&hl=en&as_sdt=0,5)
[/Citation]
[/Aside]

In early iterations, we asked students specific questions about their peer’s work, like "what would happen if you applied this peer’s argument to the Korean War instead of World War I?" Questions like these could access particularly focused ideas, but we felt a tension between the specificity of the question and the breadth of its applicability. If we got too specific, the questions wouldn’t naturally apply to many peers’ answers; if we made them universal, they would fail to scaffold specific aspects of reasoning skills.

We solved this problem by offering *several* prompts to students and asking them to choose. We framed the prompts as "sentence starters," inspired by [a common classroom practice](http://www.multibriefs.com/briefs/exclusive/using_sentence_frames.html#.Wx79B2RKjDU) in language arts and social studies classrooms. Students choose a sentence prefix like “Another geographical detail which would support this argument might be…” from a set of 3–5 and then complete the sentence. 

[Aside]
![screenshot of sentence starter selector](images/sentence-starters.png)

Sentence starters like "another piece of evidence to support this argument is…" reinforce our "low floor, high ceilings, wide walls" goal: novice students and disciplinary experts could both fruitfully write sentences beginning that way, ranging from the naive to the striking.
[/Aside]

Our open-ended activities are meant to help students develop specific complex reasoning skills. So when students engage with others’ responses, we help them view those responses *through the lens* of the relevant skills. That is: sentence starters prompt students to do something effortful with their peer’s answer which might support the activity’s target skills. For instance, if we’re trying to help a student understand historical causation, we might want them to consider whether their peer's argument includes anything about the absence of alternative causal explanations. 

Here we illustrate more examples of how we create sentence starters from skills:

[RawTable]
  [tr]
    [td][strong]Focus skill[/strong][/td]
    [td][strong]Example sentence starter[/strong][/td]
  [/tr]
  [tr]
    [td]Contextualization[/td]
    [td]One geographical influence to add might be…[/td]
  [/tr]
  [tr]
    [td]Argument development[/td]
    [td]A critic of this argument might say…[/td]
  [/tr]
  [tr]
    [td]Analysis[/td]
    [td]This claim assumes that…[/td]
  [/tr]
  [tr]
    [td]Experiment design[/td]
    [td]One source of error in this experiment might be…[/td]
  [/tr]
[/RawTable]

[Aside]
Many of our pilot iterations also used sentence starters to support *reflection* prompts in the final phase of the activity. Students wrote a summary of their experience using starters like “Now I want to review…” or “Prior to this activity, I’d assumed that….” These reflection activities didn't seem to be as helpful as other elements we're discussing, so we've deemphasized them for now.
[/Aside]

Conceptually, our working metaphor is that we "deal a hand" of sentence starters, though we don't render that visual idea literally in the interface. Students “play a card,” choosing one challenge to tackle with their peer’s response. Inspired by the physical metaphor, we experimented with the notion of “scarcity”—perhaps once a sentence starter is used once, it can’t be used again, or perhaps particularly odd sentence starters appear occasionally. We can also vary the distribution of “cards” with students’ proficiency, pushing for more nuance or precision when appropriate.

As we mentioned earlier, we emphasize the learning opportunities involved in effortfully *writing* replies to others' work over the opportunities for the recipient in *reading* those replies. That means we frame these sentence starters as a *challenge for the explicit benefit of the student writing a reply*, rather than as primarily an altruistic opportunity to help their fellow student. When possible, we offer sentence starters which we hope will create insightful replies for the recipient's benefit, but our model does not *rely* on peer feedback being helpful to its recipient.

## Peer work and model work

We saw the most promising revisions in pilot students' work when we exposed them both to their classmates’ ideas and also to expert-curated model work.

Model work allows more control. For instance, we can make sure that students see great work from an authentically engaged expert—possibly even annotating it to call out what’s particularly strong:

[Aside]
See also [our blog post](http://klr.tumblr.com/post/174644368643/curated-work-can-help-develop-student) elaborating futher on opportunities specific to model work.
[/Aside]

![Illustration of annotated model work](images/model_work_annotated-noborder.png)

We can also use model work to scaffold specific tasks, like fixing a carefully crafted weakness, or identifying which points a response would miss on an official rubric:

[AcrossAllColumns]
[MobileAltImage desktopURL:"images/scaffolded_tasks-noborder.png" mobileURL:"images/scaffolded_tasks-mobile-noborder.png" alt: "Illustration of questions like 'Which points would this answer lose?" /]
[/AcrossAllColumns]

By contrast, peer work is a bit of a lottery. The student might see some interesting ideas, or they might see nonsense. But peer work is *alive:* it situates the exchange of ideas in a social context which creates meaning for the whole activity. Interaction with peer work also creates the possibility for students to *receive* interesting personal feedback from those peers.

We wanted a social context to pervade the experience, motivating students to engage more deeply. We were inspired in part by Sandra Okita and colleagues’ memorably-named paper, "The mere belief of social interaction improves learning,"[CitationRef /] documenting a randomized controlled trial in which students who believed they were learning in a social context performed significantly better on a post-test.

[Aside][Citation]
[Okita, S. Y., Bailenson, J., & Schwartz, D. L. (2008). The mere belief of social interaction improves learning. Proceedings of the 8th International Conference on International Conference for the Learning Sciences, 2, 132–139.](https://scholar.google.com/scholar?cluster=15733667543894408870&hl=en&as_sdt=0,5)
[/Citation][/Aside]

As we changed the interface in various ways to make the social presence of classmates more apparent, we saw students drawn more deeply into the activity, and we’d hear them remarking on the social aspects of their interactions. We also saw an important feedback loop emerge: social pressure creates more student engagement, which leads to better work, which in turn creates more interesting social interactions, and so on.

In our pilots, we’ve presented students with a mix of peer work and model work—usually one example of each. The proportion could be a knob we make available to the teacher, or to the activity creator, to the student themselves, or even to an algorithm, which might emphasize peer work for disengaged students or tactical model work selections for students struggling with specific parts of the skill.

Whether it’s with peer work or model work, working with others’ ideas allows students to expand the kinds of thoughts they themselves can think. Lev Vygotsky put it best[CitationRef /]: "What a child is able to do in collaboration today he will be able to do independently tomorrow."

[Aside][Citation]
[Vygotsky, L. S. (1934). Thinking and speech (p. 210).](https://www.marxists.org/archive/vygotsky/works/words/Thinking-and-Speech.pdf)
[/Citation][/Aside]

## Dynamic matchmaking

Our design shows students work by their peers’ and experts—but *which* work should they see, and *when*? We tried many approaches, but as we iterated, a few important requirements emerged.

[Aside]
![diagram of matchmaking timeline](images/matchmaking-noborder.png)
[/Aside]

Early on, we found that students quickly disengaged anytime we made them *wait*. Many simple approaches require waiting: if the whole class moves together between "steps" of the activity (for example, from "Write" to "React"), many students will wait; if we have pairs of students swap work, one will always wait for the other. We addressed this issue by having the second student review the first, the third review the second, and so on—but it seemed inescapable that the first student to finish in a class must wait for the second. Finally, we realized that we can use model work to smooth over instances where students would be waiting for work to review. Now there’s never any waiting in the transition between the “Write" and “React” phases.

We contended with a similar problem with the transition to the "Revise" step: some students won’t have received any peer feedback yet! But as as we noted earlier, we believe much of the pedagogical value comes from writing replies to peers, not receiving them. What's more, once students have given feedback, they can see and engage with the rest of the class's responses. So students who arrive at the "Revise" step early can usefully spend their time engaging with others’ work, and revising their own work based on what they’ve learned so far. We show students incoming replies live; they can always write another revision based on new replies they receive.

In some of our experiments, we matched students with peers who met certain special criteria. For example, we asked students to take a position on an issue, then prompted them to engage with a peer who disagreed with them (specifically, we asked them to suggest extra evidence for their peer’s argument!). This extra filter complicated the matchmaking system because the distributions of students were never uniform relative to the constraints. In the example where students took positions on an issue, some views would be particularly popular, meaning that too few students were available to disagree. We solved this type of issue by dynamically asking some students to engage with additional peers and by relaxing these special constraints when the groups became too imbalanced.

[Aside]
We're also interested in exploring how matchmaking should vary based on students' level of mastery. For instance, is it better to show novices high-mastery students' work, or is low-mastery students' work more accessible and relatable? We haven't experimented with this axis yet, but it seems potentially fruitful.
[/Aside]

## Social interactions

While the social engagement between students really does encourage students to participate more deeply, a social environment can also have a profoundly negative effect on students. Bullying or abuse can ruin the experience; an encounter with an apathetic peer can dampen one’s own excitement. Within our pilots, we relied on teachers to set social norms and moderate interactions, but we’ll need to develop systems to address these problems more seriously indeed as we scale up—particularly when opening the door to learners outside classrooms.

[Aside]
While it was not the focus of our investigation to date, we've also begun thinking about how we might extend this framework to people studying independently of a formal classroom environment. Our pilot independent learners valued the exposure to others' ideas, but we'll want to carefully and deeply investigate how we might build a successful shared social environment for independent learners tackling these activities. You can read some early thinking [on our blog](http://klr.tumblr.com/post/158814741858/safely-showing-students-how-others-see-their-work).
[/Aside]

Besides the deeper interactions involving sentence starters, we gave students simple reactions like a "gold star" (to give a peer props for great work) and “thumbs down” (to indicate that a submission might need a teacher’s attention). Students used these interactions frequently as soon we introduced them. In the future, we’re excited to explore richer ways for students to respond to each other: stickers that can be placed contextually within a peer’s work; inline commenting; highlighters, perhaps with special meaning for each color.

[Aside]
![screenshot of tooltip on thumbs down button](images/voting.png)
[/Aside]

One important question here has been: should students remain anonymous, or not? We’ve heard conflicting answers from teachers and students. The reality is probably fluid and context-sensitive. For instance, students tell us they’re more comfortable writing critical feedback (and receiving it!) when they know they’re anonymous. Many teachers have told us that anonymity can help avoid abusive or disruptive situations. On the other hand, some teachers have noted that sometimes de-anonymizing can imbue a helpful sense of accountability and responsibility. Our pilots made use of breakfast foods as pseudonyms; in the future, we'll likely end up letting teachers choose whether to show student names.

The social environment lives on once the activity’s nominally over. We invite students to continue the conversation indefinitely if they like—we thought of this phase like an afterparty. We saw some of our pilot students take us up on that offer, interacting with each others’ ideas long after their class was over.

[Aside][Citation]
[Slavin, R. E. (1996). Research on cooperative learning and achievement: What we know, what we need to know. Contemporary Educational Psychology, 21, 43–69.](https://scholar.google.com/scholar?cluster=7317953963464845567&hl=en&as_sdt=0,5)
[/Citation][/Aside]

We hope that as we continue improving the structures for discourse in these activities, we’ll help students build important metacognitive skills while they build topical reasoning skills[CitationRef /]: giving and receiving feedback, interpreting others’ ideas, revising work iteratively.

## Real-time reports for teachers

In addition to directly supporting students in developing their understanding, elements of our activities also offer insights to teachers about their students’ thinking. Teachers might use that information in 1:1 coaching, when forming small groups, or to guide whole-class instruction.

One challenge of these activities is that they generate a huge amount of data. We don’t expect many teachers to read through it all. It’s important that we try to surface the information that’s most likely to be important, while also leaving them control to read more deeply for specific students they’re interested in.

This led us to develop a real-time report that shows student thought and highlights important information.

[Aside]
In the report screenshot below, students' names have been anonymized.
[/Aside]

[AcrossAllColumns]
![Screenshot of teacher report](images/report.png)
[/AcrossAllColumns]

The report has a few key features:

Highlighted revisions make student thought highly visible and easy to engage with. For many teachers, the change in understanding between when the student began the activity and when they finished it is the thing they want to see first. Scanning over the report, they can quickly spot where students made significant changes (and where other students may have made more perfunctory changes).

Social interactions in aggregate draw teachers’ attention to submissions which may merit more attention. The “gold star” might highlight students whose response to the prompt or reply to a peer might be interesting to share with the class, while the “thumbs down” mechanism can quickly surface students who might benefit from 1:1 support.

[Aside]
![Screenshot of teacher report, zoomed in on students' votes and confidence measures](images/report-zoom.png)
[/Aside]

Confidence, which students report privately by selecting an emoji after submitting their draft and each revision, helps teachers follow up with particular students who may be challenged in ways that are not immediately visible from their work. The sequence of students' confidence ratings are displayed next to their names in the report.

We found that teachers use the report in multiple contexts. They use it in-class to monitor student participation. They skim it after class to ensure that students completed the activity, and to spot-check particular students’ submissions. Pilot teachers tell us they’re not really interested in reading deeply into every students’ work in these kind of lightweight activities: they want some assurances that the activity will help students, they want to see that everyone actually completed it, and they want to focus on a few top highlights to steer their immediate actions.

There are opportunities for reports to further assist teacher facilitation: sorting functionality to surface responses that need intervention, or are particularly meaty for discussion; the ability for a teacher to "drop in" on any response to provide feedback of their own; lightweight text analysis to spot trends in class writing or surface problematic work. In all cases, we seek to make reports useful for facilitating activities and understanding how they’re going, rather than delivering individual grades for participants.

Teachers of Advanced Placement (AP) courses routinely involve rubrics in their class activities—not to provide students with a grade, but to offer a scaffold. The rubrics help students understand what’s expected of them and function like a checklist, a series of lenses they can use to think about their ideas or others’.

Though we haven’t tested it yet, we believe that it would be helpful to offer students a "checklist" they can use to evaluate their own work or others’. This might even feed into the sentence starters. For instance, if a student says they think they’ve appropriately contextualized their evidence, it’s interesting for a peer to consider what else might be added to the contextualization—not so much to validate the score as to foster useful thought.

## Activities that scale

One thing that prevents classrooms from running more open-ended activities is that it’s nearly impossible for teachers to deliver rapid feedback to all their students. We’ve shown how our platform helps solve that problem through peer interactions and the teacher report.

Still another challenge is the difficulty of authoring activities in the first place. A good activity is carefully constructed to employ a particular skill around a given stimulus—something like a historical source, graph, or laboratory experiment. Many teachers rely on questions from standardized exams—but there are usually so few that they get saved for summative tests and quizzes.

Happily, we found that within our prototype platform, open-ended activities can at once be rich and relatively inexpensive to author. At first glance, this seems unlikely. The activities in our framework require more content than a typical exam question, since they make use of model work and sentence starters. But because the activities focus on a reasonably small number of complex reasoning skills, they can be created at scale.

First, a single stimulus can be "pivoted" to create multiple activities, each one focusing on a specific skill. Consider this example from AP US History:

![Screenshot of AP US History skills applied to a stimulus](images/progressivism-noborder.png)

In this way, we are able to construct four activities at once, all based on the same historical document.

Exemplary responses are relatively easy for our instructional designers to author. But we can envision a solution where exemplary responses emerge organically from students themselves; peer work that received many upvotes could potentially be "promoted" to model work and be seen by more students.

Sentence starters, which are critical for scaffolding peer feedback, do not actually need to be created fresh per activity. Rather, they need to be created per *skill*. For example, the sentence starter "A more significant cause would be…" will always be useful for any activity that aims to develop the reasoning skill of causation.

## Many modes of expression

While many of our pilots asked students to write prose in a text box, we also experimented with a variety of other input methods. Some broadened the types of problems students could express; others captured more nuance in their thinking; still others focused on supporting the social environment.

We particularly enjoyed offering students the ability to draw freeform ink. For math activities, we layered an interactive scratchpad on top of figures we provided so that students could show their thinking in context.

![Handwritten student math on top of a mathematical diagram](images/handwriting.png)

[Aside]
In this problem, students were asked to draw how they might find the area of this trapezoid.
[/Aside]

For the freehand inking tool, we record students’ full gesture timing so that we could represent their process to others later. When we show others’ drawings to students in the interface, we depict that work as an animation—a more dynamic representation of a thought process. We noticed that many students were more engaged by others’ ideas when they were represented this way.

![Animation of student handwriting on top of a mathematical diagram](images/animated_writing.gif)

[Aside]
We created early sketches of a large variety of input modes: speech, stickers, highlighting, and more. [This blog post](http://klr.tumblr.com/post/157770095858/feedback-is-a-gift) includes about on our experiments with other input modalities.
[/Aside]

# 5. In class, after class, and at home: a vignette

We interviewed teachers to understand how they currently find the time in their school year to run open-ended activities. In large part because they're so burdensome to conduct, we heard many teachers say they often cram them in toward the end of the school year, when preparing for exams. In this context, open-ended activities usually simulate the conditions of the free response portions of the exam. They’re likely timed and conducted in isolation.

[Aside]
![drawing of timeline depicting teachers pushing open-ended work to the end of the year](images/open-ended-timeline-noborder.png)
[/Aside]

How might this change if open-ended activities were lighter, social, and easier to run? When we showed teachers our prototype, they were full of ideas about how to use it. Teachers saw themselves running activities to preview a new topic; to check for understanding of the night’s reading; to run an end-of-class reflection; to practice essay-writing in smaller chunks. This diverse set of use cases inspired us to design a flexible system that can be run in-class or asynchronously at home; that can be as quick as a few minutes or take an hour; that can facilitate moderation but does not strictly require it.

[Aside]
Learn more about the process we used to identify teachers’ top challenges and connect them to concept generation through "baseball cards" [on our blog](http://klr.tumblr.com/post/174256565853/sorting-product-baseball-cards).

![drawing of students talking to people outside their bubble](images/break-bubble-noborder.png)
[/Aside]

Teachers shared our interest in using online activities to expose students to voices and ideas outside their classroom, neighborhood, even country.  With a common goal of setting their students up to become global citizens who can engage in civil discourse, many teachers were eager for students to—as one put it—"get out of their bubble."

What follows is a summary of an observation we did with a pilot classroom (with names changed). Rather than being representative of a particular use case, it aims instead to illustrate how fluid classroom environments can be, and how our solution accommodates them.

[SemiHairline /]

Mr. Appleseed is an AP World History teacher at Monterey High. He teaches AP World History to 150 students across 6 periods. 

[Aside]
![drawing of Mr. Appleseed at the front of his class](images/mr-appleseed-noborder.png)
[/Aside]

With the AP exam looming, Mr. Appleseed is reviewing the big ideas from earlier periods with his class. In the final 30 minutes, Mr. Appleseed asks students to open up an open-ended activity on their Chromebooks. The activity's prompt asks:

*Between 10,000 B.C.E. and 600 B.C.E., the adoption of agriculture had significant social, economic, and demographic effects.*

*In 5–8 sentences, develop an argument that evaluates how the adoption of agriculture in this time period affected the development of human societies. Be sure to include specific factual evidence supporting both what changed and what stayed the same*.

This activity requires the application of complex reasoning skills. It requires students to develop an argument, which they'll need to do for essay portion of the upcoming exam. And it asks for the reasoning skill of *Continuity and change over time*, a key disciplinary practice in the AP Histories that underpins the entire course.

Mr. Appleseed has students pair up to discuss the prompt. As they do so, he calls their attention to particular details: "A great response would include some proper nouns, like the Neolithic Revolution", or "I don't want to see any of that 'There were a variety of changes'... junk in your thesis."

[Aside]
![drawing of Mr. Appleseed at the front of his class](images/student-pairs-noborder.png)
[/Aside]

After a few minutes, Mr. Appleseed has the students begin writing their responses. He guides the class along the activity with lightweight time management: "Spend about 5-7 minutes drafting a response, just get something out there." "With your replies to other students, go above and beyond—you're doing them a big favor."

As students work through the activity, Mr. Appleseed splits his time between the projector, where he is browsing student responses, and roaming the class, where he occasionally asks a student a question or provides help where it's needed. Struck by something he's noticed, he pulls up one student's response on the projector and asks the class to weigh in on it. "Let's see a thumbs up or down: is this a specific enough historical claim?" The quick consensus is that it is. Mr. Appleseed agrees, and encourages the class to make sure they substantiate their claim in their response.

[Aside]
![Illustration of a teacher monitoring student work on their machine](images/appleseed-monitoring-noborder.png)
[/Aside]

As the period comes to an end, some students are still waiting for replies, while others have completed multiple revisions. "Share whatever you have at this point," Mr. Appleseed advises. "This is just a working draft." Typing stops and Mr. Appleseed asks who can share how feedback changed their thinking. Hannah raises her hand:

[Aside]
![Illustration of a student reading a note their partner sent them](images/partner-commentary-noborder.png)
[/Aside]

"My partner said: ‘This response could use the same evidence to support its thesis better by expanding on the ideas and explaining exactly how life changes as a result of Agriculture.’ So I added: ‘What allowed these cities to grow is that since they farmed, they had a surplus of food. With the surplus it allowed people not to become farmers and start specializing in certain works. This led to technology advancements and a stronger economy. Since every civilization had different products they would trade with other civilizations.’"

Immediately after class, Mr. Appleseed spends some time spot-checking the report and reminding students who didn’t yet revise their response to do so. Later, he tells us: "I'm noticing more nuances. For instance, I need to train students that the reply, "a critic of this argument..." is meant to challenge the substance of an argument and help students engage in a scholarly discussion rather than simply writing, "a critic of this argument would note that you did not use enough evidence." Many of my students were using the latter type of reply without really engaging in argumentation. This is already providing great data on my students strengths & areas of growth."

[Aside]
![Illustration of Mr. Appleseed checking over the report after class](images/appleseed-checks-report-noborder.png)
[/Aside]

Feedback from pilot teachers like "Mr. Appleseed," along with our own explorations, provided us with an exciting number of possibilities to further extend the platform.

# 6. Open-ended inquiry, every day

We aspire to support students as they develop into critical thinkers and empowered practitioners, ready to contend with vivid challenges in their disciplines and their societies. Yes, they’ll need facts, and yes, they’ll need to perform routine procedures—but they’ll also bring independent thought and creativity to questions with no straightforward answers. With any luck, they’ll not tackle those future problems alone: as practitioners, our students will exchange ideas fluidly with others in their discipline, including with others who might challenge their own perspectives and strategies.

[Aside]
![illustration of student thinking in many disciplines /](images/exchanging-ideas-noborder.png)
[/Aside]

Students shouldn’t have to wait until their adulthood to pursue open-ended problems and improve their ideas through discourse with others. Indeed, as we’ve discussed, contemporary learning standards insist that students develop these skills earlier than ever. Skillfully-orchestrated classroom activities can successfully build these practices, but teachers across the country have lamented to us about how challenging and time-consuming these sessions are to facilitate.

What if online learning platforms could deliver rapid, reliable, and relevant feedback to students on complex open-ended activities, the way they already do on simpler multiple-choice exercises? What if these activities could facilitate thoughtful and energetic discourse between students and their differing ideas?

With such a platform, students wouldn’t merely acquire the complex reasoning skills they need to pass the next test. Better: their schoolwork would emphasize original, creative, critical thought. Every day, they would participate in lightweight, meaningful, and social discussion about open-ended questions. 

In other words—speaking more aspirationally—their educational experience would look a lot like the experience of professionals in their fields.

[Aside]
![illustration of student thinking in many disciplines /](images/in-the-discipline-noborder.png)
[/Aside]

Like mathematicians, math students would notice and communicate patterns underlying and connecting systems. Like scientists, science students would form conjectures and create models. And like historians, history students would analyze original documents to interpret the processes of civilization.  

We believe it’s possible for online learning platforms to support teachers and students in open-ended inquiry. We’re excited to have illustrated some possible paths with our experiments thus far, and we’re looking forward to developing these ideas further in future projects.

# 7. Acknowledgements

We're grateful to Tabitha Yong, who spent several weeks collaborating with us at the close of this project; she contributed several illustrations in this report, thoughtful visual advice, and a valuable storytelling perspective.

Weʼd like to thank the following people for their valuable thoughts on this report or the investigation it describes: Ari Bader-Natal, Leonard Bogdonoff, Kathleen Chung, Elena Glassman, Eli Luberoff, Ryan Mather, Dan Meyer, Michael Nielsen, Benjamin Reinhardt, Jack Schaedler, Brendan Schlagel, Paul Simeon, Maria Zavala.

So many of our colleagues at Khan Academy helped us during this project, but we'd like to express particular gratitude to contributions from Sean Boston, Cam Christenson, Kim Kutz Elliott, Dave Herron, Jason Hovey, Sal Khan, Ginny Lee, Sarah Lim, Leah Marquez, Caroline Pritchard, David Rheinstrom, April Russell, and Allison Zimmerman.

We'd also like to extend our sincere gratitude to the many teachers and students who participated in our interviews, created activities with us, and used their valuable class time to run a pilot with us. We promised all participants their privacy by default, but these teachers gave us permission to acknowledge them by name, which we'll do now with much appreciation: Christina Braswell, Neal Cates, Tracey Delyea, Andrea Glenn, Nathan Gong, Phil Grebe, Emily Macy, Jacqueline McNeilly, Olivia Morales, Diane Morey, Timothy Mulvehill, Cara Rosenthal, Paul Sargent, Jason Searle, Jah-Yee Woo, Jenna Wright.

While comments from these people have hugely improved this report, any remaining deficiencies in this work should be attributed to the authors alone. The people listed here should not necessarily be construed as endorsing this report.

We are grateful to General Motors for their support of this project.

[Hairline /]

In academic work, please cite this as:

> Nick Barr, Scott Farrar, Brian Johnsrud, May-Li Khoe, Andy Matuschak. (2018, August 24). *Building complex reasoning skills online through open-ended activities.* https://early.khanacademy.org/open-ended.

[Aside]
Want to discuss this work with us? Please feel free to [email the authors](mailto:early-product-development-team@khanacademy.org) or [follow us on Twitter](https://twitter.com/andy_matuschak/lists/open-ended-project-team/members).
[/Aside]

This report is licensed under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).